import numpy as np
import cv2
import pickle
import os
import torch
import torch.nn.functional as F
from insightface.app import FaceAnalysis
import sys
from torch import nn
sys.path.append('/home/wenhao/CUHK/ECE4513/FinalProject/PreProcess/MotionBlur')
from DeConvModel import TinyFreqNet

# åˆå§‹åŒ– InsightFace æ¨¡å‹ï¼ˆä½¿ç”¨ GPUï¼‰
app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider'])
app.prepare(ctx_id=0, det_size=(640, 640))

# åŠ è½½é€†å·ç§¯æ¨¡å‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
deblur_model = TinyFreqNet(kernel_size=25)  # è¾“å‡º25x25å·ç§¯æ ¸
checkpoint = torch.load('/home/wenhao/CUHK/ECE4513/FinalProject/training_output/models/best_model.pth', map_location=device)
deblur_model.load_state_dict(checkpoint['model_state_dict'])
deblur_model.to(device)
deblur_model.eval()

# æ›´æ¿€è¿›çš„æ£€æµ‹é˜ˆå€¼ä¼˜åŒ–
for model in app.models.values():
    if hasattr(model, 'det_thresh'):
        model.det_thresh = 0.15  # è¿›ä¸€æ­¥é™ä½åˆ°0.15
    if hasattr(model, 'nms_thresh'):
        model.nms_thresh = 0.2   # æ›´ä½çš„NMSé˜ˆå€¼


def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def wiener_deconvolution(blurred_img, kernel, noise_var=0.01):
    """
    ä½¿ç”¨ç»´çº³æ»¤æ³¢è¿›è¡Œé€†å·ç§¯å»æ¨¡ç³Š
    
    Args:
        blurred_img: æ¨¡ç³Šå›¾åƒ (H, W) æˆ– (H, W, C)
        kernel: è¿åŠ¨æ¨¡ç³Šå·ç§¯æ ¸ (25, 25)
        noise_var: å™ªå£°æ–¹å·®
    
    Returns:
        restored_img: å»æ¨¡ç³Šåçš„å›¾åƒ
    """
    if len(blurred_img.shape) == 3:
        # å½©è‰²å›¾åƒï¼Œåˆ†åˆ«å¤„ç†æ¯ä¸ªé€šé“
        restored_channels = []
        for c in range(blurred_img.shape[2]):
            channel = blurred_img[:, :, c]
            restored_channel = wiener_deconvolution_single_channel(channel, kernel, noise_var)
            restored_channels.append(restored_channel)
        return np.stack(restored_channels, axis=2)
    else:
        # ç°åº¦å›¾åƒ
        return wiener_deconvolution_single_channel(blurred_img, kernel, noise_var)

def wiener_deconvolution_single_channel(blurred_channel, kernel, noise_var):
    """
    å•é€šé“ç»´çº³æ»¤æ³¢å»å·ç§¯
    """
    # å›¾åƒå’Œå·ç§¯æ ¸çš„å‚…é‡Œå¶å˜æ¢
    blurred_fft = np.fft.fft2(blurred_channel)
    
    # å°†25x25å·ç§¯æ ¸æ‰©å±•åˆ°å›¾åƒå¤§å°
    h, w = blurred_channel.shape
    kernel_padded = np.zeros((h, w))
    kh, kw = kernel.shape
    start_h, start_w = (h - kh) // 2, (w - kw) // 2
    kernel_padded[start_h:start_h + kh, start_w:start_w + kw] = kernel
    
    kernel_fft = np.fft.fft2(kernel_padded)
    
    # ç»´çº³æ»¤æ³¢
    kernel_conj = np.conj(kernel_fft)
    kernel_mag_sq = np.abs(kernel_fft) ** 2
    
    # ç»´çº³æ»¤æ³¢å™¨
    wiener_filter = kernel_conj / (kernel_mag_sq + noise_var)
    
    # åº”ç”¨æ»¤æ³¢å™¨
    restored_fft = blurred_fft * wiener_filter
    restored_img = np.real(np.fft.ifft2(restored_fft))
    
    # è£å‰ªåˆ°åˆç†èŒƒå›´
    restored_img = np.clip(restored_img, 0, 255)
    
    return restored_img

def predict_kernel_and_deblur(img):
    """
    ä½¿ç”¨TinyFreqNeté¢„æµ‹å·ç§¯æ ¸å¹¶è¿›è¡Œå»æ¨¡ç³Š
    
    Args:
        img: è¾“å…¥çš„æ¨¡ç³Šå›¾åƒ (BGRæ ¼å¼)
    
    Returns:
        deblurred_img: å»æ¨¡ç³Šåçš„å›¾åƒ
        predicted_kernel: é¢„æµ‹çš„25x25å·ç§¯æ ¸
    """
    # è½¬æ¢ä¸ºç°åº¦å›¾å¹¶å½’ä¸€åŒ–
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img
    
    # è°ƒæ•´åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸ (112, 96)
    resized = cv2.resize(gray, (96, 112))
    normalized = resized.astype(np.float32) / 255.0
    
    # è½¬æ¢ä¸ºå¼ é‡
    input_tensor = torch.from_numpy(normalized).unsqueeze(0).unsqueeze(0).to(device)
    
    # é¢„æµ‹å·ç§¯æ ¸ï¼ˆé¢‘åŸŸï¼‰
    with torch.no_grad():
        kernel_fft_pred = deblur_model(input_tensor)  # [1, 25, 25] complex
    
    # è½¬æ¢åˆ°ç©ºé—´åŸŸ
    # æ–°çš„ç½‘ç»œç›´æ¥è¾“å‡ºé¢‘åŸŸçš„å·ç§¯æ ¸ï¼Œéœ€è¦è½¬æ¢åˆ°ç©ºé—´åŸŸ
    kernel_spatial = torch.real(torch.fft.ifft2(kernel_fft_pred)).squeeze().cpu().numpy()
    
    # æ£€æŸ¥å·ç§¯æ ¸æ˜¯å¦æœ‰æ•ˆ
    kernel_sum = np.sum(np.abs(kernel_spatial))
    if kernel_sum > 1e-8:
        # å½’ä¸€åŒ–å·ç§¯æ ¸
        kernel_spatial = kernel_spatial / kernel_sum
    else:
        # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å·ç§¯æ ¸
        print("Warning: Predicted kernel is invalid, using default kernel")
        kernel_spatial = np.zeros((25, 25))
        kernel_spatial[12, 12] = 1.0
    
    # ç¡®ä¿å·ç§¯æ ¸ä¸ºæ­£å€¼ï¼ˆç‰©ç†çº¦æŸï¼‰
    kernel_spatial = np.abs(kernel_spatial)
    kernel_spatial = kernel_spatial / (np.sum(kernel_spatial) + 1e-8)
    
    # ä½¿ç”¨ç»´çº³æ»¤æ³¢è¿›è¡Œå»æ¨¡ç³Š
    img_float = img.astype(np.float32)
    deblurred_img = wiener_deconvolution(img_float, kernel_spatial, noise_var=0.01)
    
    # è½¬æ¢å›uint8
    deblurred_img = np.clip(deblurred_img, 0, 255).astype(np.uint8)
    
    return deblurred_img, kernel_spatial

def evaluate(test_dir='Data/CASIA', gallery_path='gallery.pkl', threshold=0.35):
    with open(gallery_path, 'rb') as f:
        gallery = pickle.load(f)

    correct = 0
    total = 0
    for person in os.listdir(test_dir):
        person_path = os.path.join(test_dir, person)
        if not os.path.isdir(person_path):
            continue
        test_img_path = os.path.join(person_path, '001.jpg')
        if not os.path.exists(test_img_path):
            if 1:
                # å¦‚æœæ—‹è½¬æ•°æ®æ²¡æœ‰001.jpgï¼Œå°è¯•è¯»å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„å›¾ç‰‡
                img_files = [f for f in os.listdir(person_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if not img_files:
                    print(f"[!] {person}: æ²¡æœ‰å¯ç”¨å›¾ç‰‡")
                    continue
                test_img_path = os.path.join(person_path, img_files[0])
        img = cv2.imread(test_img_path)
        if img is None:
            print(f"[!] {person}: æ— æ³•è¯»å–å›¾åƒ")
            continue
        
        # å…ˆè¿›è¡Œé€†å·ç§¯å»æ¨¡ç³Šå¤„ç†
        deblurred_img, predicted_kernel = predict_kernel_and_deblur(img)
        
        # å¤šå°ºåº¦æ£€æµ‹ç­–ç•¥ï¼ˆä½¿ç”¨å»æ¨¡ç³Šåçš„å›¾åƒï¼‰
        faces = []
        
        # ç­–ç•¥1: å¯¹äºå°å›¾åƒï¼Œå…ˆæ”¾å¤§åˆ°åˆé€‚çš„å°ºå¯¸
        if deblurred_img.shape[0] < 224 or deblurred_img.shape[1] < 224:
            # å°è¯•å¤šç§æ”¾å¤§å€æ•°
            scale_factors = [
                max(300 / deblurred_img.shape[0], 300 / deblurred_img.shape[1]),  # åŸç­–ç•¥
                max(400 / deblurred_img.shape[0], 400 / deblurred_img.shape[1]),  # æ›´å¤§æ”¾å¤§
                max(224 / deblurred_img.shape[0], 224 / deblurred_img.shape[1])   # ä¿å®ˆæ”¾å¤§
            ]
            
            for scale_factor in scale_factors:
                new_h = int(deblurred_img.shape[0] * scale_factor)
                new_w = int(deblurred_img.shape[1] * scale_factor)
                resized_img = cv2.resize(deblurred_img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
                
                detected_faces = app.get(resized_img)
                if len(detected_faces) > 0:
                    faces = detected_faces
                    break
        
        # ç­–ç•¥2: å¦‚æœæ”¾å¤§åä»ç„¶æ£€æµ‹ä¸åˆ°ï¼Œå°è¯•åŸå›¾
        if len(faces) == 0:
            faces = app.get(deblurred_img)
        
        # ç­–ç•¥3: å¦‚æœè¿˜æ˜¯æ£€æµ‹ä¸åˆ°ï¼Œå°è¯•å›¾åƒå¢å¼º
        if len(faces) == 0:
            # å¢å¼ºå¯¹æ¯”åº¦
            enhanced_img = cv2.convertScaleAbs(deblurred_img, alpha=1.5, beta=30)
            if enhanced_img.shape[0] < 224 or enhanced_img.shape[1] < 224:
                scale_factor = max(300 / enhanced_img.shape[0], 300 / enhanced_img.shape[1])
                new_h = int(enhanced_img.shape[0] * scale_factor)
                new_w = int(enhanced_img.shape[1] * scale_factor)
                enhanced_img = cv2.resize(enhanced_img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
            
            faces = app.get(enhanced_img)
        
        if len(faces) == 0:
            print(f"[!] {person}: æœªæ£€æµ‹åˆ°äººè„¸ (å·ç§¯æ ¸èŒƒæ•°: {np.linalg.norm(predicted_kernel):.4f})")
            continue
        query_emb = faces[0].normed_embedding

        best_name, best_score = None, -1
        for name, emb_list in gallery.items():
            sims = [cosine_similarity(query_emb, emb) for emb in emb_list]
            max_sim = max(sims)
            if max_sim > best_score:
                best_score = max_sim
                best_name = name

        is_correct = (best_name == person) and (best_score >= threshold)
        result = "âœ“" if is_correct else "âœ—"
        kernel_norm = np.linalg.norm(predicted_kernel)
        print(f"[{result}] é¢„æµ‹: {best_name:10} åˆ†æ•°: {best_score:.3f} â†’ çœŸå®: {person} (æ ¸èŒƒæ•°: {kernel_norm:.4f})")
        if is_correct:
            correct += 1
        total += 1

    acc = correct / total if total > 0 else 0
    print(f"\nğŸ¯ è¯†åˆ«å‡†ç¡®ç‡: {acc*100:.2f}% ï¼ˆ{correct}/{total}ï¼‰")

if __name__ == "__main__":
    evaluate(test_dir='Data/CASIA_MotionBlurred', gallery_path='gallery.pkl', threshold=0.35)
