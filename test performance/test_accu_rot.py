import numpy as np
import cv2
import pickle
import os
import torch
import torch.nn.functional as F
from insightface.app import FaceAnalysis
import sys
sys.path.append('/home/wenhao/CUHK/ECE4513/FinalProject/PreProcess/Rotation')
from RotationModel import RotationModel

# åˆå§‹åŒ– InsightFace æ¨¡å‹ï¼ˆä½¿ç”¨ GPUï¼‰
app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider'])
app.prepare(ctx_id=0, det_size=(640, 640))

# åŠ è½½æ—‹è½¬é¢„æµ‹æ¨¡å‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
rotation_model = RotationModel(image_zise=(112, 96))
checkpoint = torch.load('/home/wenhao/CUHK/ECE4513/FinalProject/best_rotation_model.pth', map_location=device)
rotation_model.load_state_dict(checkpoint['model_state_dict'])
rotation_model.to(device)
rotation_model.eval()

# æ›´æ¿€è¿›çš„æ£€æµ‹é˜ˆå€¼ä¼˜åŒ–
for model in app.models.values():
    if hasattr(model, 'det_thresh'):
        model.det_thresh = 0.15  # è¿›ä¸€æ­¥é™ä½åˆ°0.15
    if hasattr(model, 'nms_thresh'):
        model.nms_thresh = 0.2   # æ›´ä½çš„NMSé˜ˆå€¼


def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def crop_black_borders(img, tolerance=10):
    """è£å‰ªå›¾åƒçš„é»‘è¾¹éƒ¨åˆ†"""
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img
    
    # æ‰¾åˆ°éé»‘è‰²åƒç´ çš„è¾¹ç•Œ
    coords = np.column_stack(np.where(gray > tolerance))
    if len(coords) == 0:
        return img  # å¦‚æœå…¨æ˜¯é»‘è‰²ï¼Œè¿”å›åŸå›¾
    
    # è·å–è¾¹ç•Œæ¡†
    y_min, x_min = coords.min(axis=0)
    y_max, x_max = coords.max(axis=0)
    
    # è£å‰ªå›¾åƒ
    if len(img.shape) == 3:
        cropped = img[y_min:y_max+1, x_min:x_max+1, :]
    else:
        cropped = img[y_min:y_max+1, x_min:x_max+1]
    
    return cropped

def predict_rotation_and_correct(img):
    """é¢„æµ‹å›¾åƒæ—‹è½¬è§’åº¦å¹¶è¿›è¡Œæ—‹è½¬æ ¡æ­£"""
    # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img
    
    # è°ƒæ•´å›¾åƒå¤§å°åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸
    resized = cv2.resize(gray, (96, 112))
    
    # å½’ä¸€åŒ–
    normalized = resized.astype(np.float32) / 255.0
    
    # è½¬æ¢ä¸ºå¼ é‡
    tensor = torch.from_numpy(normalized).unsqueeze(0).unsqueeze(0).to(device)
    
    # é¢„æµ‹æ—‹è½¬è§’åº¦
    with torch.no_grad():
        predicted_angle = rotation_model(tensor).item()
    
    # å¦‚æœé¢„æµ‹è§’åº¦æ¥è¿‘0ï¼Œç›´æ¥è¿”å›åŸå›¾
    if abs(predicted_angle) < 1.0:
        return img, predicted_angle
    
    # æ—‹è½¬æ ¡æ­£
    h, w = img.shape[:2]
    center = (w // 2, h // 2)
    rotation_matrix = cv2.getRotationMatrix2D(center, -predicted_angle, 1.0)
    corrected_img = cv2.warpAffine(img, rotation_matrix, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))
    
    # è£å‰ªé»‘è¾¹
    corrected_img = crop_black_borders(corrected_img, tolerance=10)
    
    return corrected_img, predicted_angle

def evaluate(test_dir='Data/CASIA', gallery_path='gallery.pkl', threshold=0.35):
    with open(gallery_path, 'rb') as f:
        gallery = pickle.load(f)

    correct = 0
    total = 0
    for person in os.listdir(test_dir):
        person_path = os.path.join(test_dir, person)
        if not os.path.isdir(person_path):
            continue
        test_img_path = os.path.join(person_path, '001.jpg')
        if not os.path.exists(test_img_path):
            if test_dir == 'Data/CASIA_Rotated':
                # å¦‚æœæ—‹è½¬æ•°æ®æ²¡æœ‰001.jpgï¼Œå°è¯•è¯»å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„å›¾ç‰‡
                img_files = [f for f in os.listdir(person_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if not img_files:
                    print(f"[!] {person}: æ²¡æœ‰å¯ç”¨å›¾ç‰‡")
                    continue
                test_img_path = os.path.join(person_path, img_files[0])
        img = cv2.imread(test_img_path)
        if img is None:
            print(f"[!] {person}: æ— æ³•è¯»å–å›¾åƒ")
            continue
        
        # å…ˆè¿›è¡Œæ—‹è½¬å¤åŸ
        corrected_img, predicted_angle = predict_rotation_and_correct(img)
        
        # å¤šå°ºåº¦æ£€æµ‹ç­–ç•¥ï¼ˆä½¿ç”¨æ—‹è½¬æ ¡æ­£åçš„å›¾åƒï¼‰
        faces = []
        
        # ç­–ç•¥1: å¯¹äºå°å›¾åƒï¼Œå…ˆæ”¾å¤§åˆ°åˆé€‚çš„å°ºå¯¸
        if corrected_img.shape[0] < 224 or corrected_img.shape[1] < 224:
            # å°è¯•å¤šç§æ”¾å¤§å€æ•°
            scale_factors = [
                max(300 / corrected_img.shape[0], 300 / corrected_img.shape[1]),  # åŸç­–ç•¥
                max(400 / corrected_img.shape[0], 400 / corrected_img.shape[1]),  # æ›´å¤§æ”¾å¤§
                max(224 / corrected_img.shape[0], 224 / corrected_img.shape[1])   # ä¿å®ˆæ”¾å¤§
            ]
            
            for scale_factor in scale_factors:
                new_h = int(corrected_img.shape[0] * scale_factor)
                new_w = int(corrected_img.shape[1] * scale_factor)
                resized_img = cv2.resize(corrected_img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
                
                detected_faces = app.get(resized_img)
                if len(detected_faces) > 0:
                    faces = detected_faces
                    break
        
        # ç­–ç•¥2: å¦‚æœæ”¾å¤§åä»ç„¶æ£€æµ‹ä¸åˆ°ï¼Œå°è¯•åŸå›¾
        if len(faces) == 0:
            faces = app.get(corrected_img)
        
        # ç­–ç•¥3: å¦‚æœè¿˜æ˜¯æ£€æµ‹ä¸åˆ°ï¼Œå°è¯•å›¾åƒå¢å¼º
        if len(faces) == 0:
            # å¢å¼ºå¯¹æ¯”åº¦
            enhanced_img = cv2.convertScaleAbs(corrected_img, alpha=1.5, beta=30)
            if enhanced_img.shape[0] < 224 or enhanced_img.shape[1] < 224:
                scale_factor = max(300 / enhanced_img.shape[0], 300 / enhanced_img.shape[1])
                new_h = int(enhanced_img.shape[0] * scale_factor)
                new_w = int(enhanced_img.shape[1] * scale_factor)
                enhanced_img = cv2.resize(enhanced_img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
            
            faces = app.get(enhanced_img)
        
        if len(faces) == 0:
            print(f"[!] {person}: æœªæ£€æµ‹åˆ°äººè„¸ (æ—‹è½¬è§’åº¦: {predicted_angle:.1f}Â°)")
            continue
        query_emb = faces[0].normed_embedding

        best_name, best_score = None, -1
        for name, emb_list in gallery.items():
            sims = [cosine_similarity(query_emb, emb) for emb in emb_list]
            max_sim = max(sims)
            if max_sim > best_score:
                best_score = max_sim
                best_name = name

        is_correct = (best_name == person) and (best_score >= threshold)
        result = "âœ“" if is_correct else "âœ—"
        print(f"[{result}] é¢„æµ‹: {best_name:10} åˆ†æ•°: {best_score:.3f} â†’ çœŸå®: {person} (æ—‹è½¬: {predicted_angle:.1f}Â°)")
        if is_correct:
            correct += 1
        total += 1

    acc = correct / total if total > 0 else 0
    print(f"\nğŸ¯ è¯†åˆ«å‡†ç¡®ç‡: {acc*100:.2f}% ï¼ˆ{correct}/{total}ï¼‰")

if __name__ == "__main__":
    evaluate(test_dir='Data/CASIA_Rotated', gallery_path='gallery.pkl', threshold=0.35)
